---
title: "CityFibre - 1. Data Understanding Lesson"
output:
  html_notebook:
    highlight: pygments
    number_sections: yes
    theme: united
    toc: yes
    toc_float: yes
  html_document:
    toc: yes
    df_print: paged
---

<div class="alert alert-block alert-success">

**Part 1 - Data Understanding**

This notebook steps through the first part of the CityFibre data analysis problem.
This involves: 

* Setting up the computer environment with the required supporting packages
* Reading in the datasets.
* Tidying up the variable names in each dataset so that they are easier to work with.
* Acquire an initial understanding of each dataset size and shape and the types and format of the data that they contain.
* Identify any obvious data quality problems that might need to be addressed.
* Finally, gain an insight into how the columns are related to each other.

Where the code contains **s around the text, that is where students will need to change the text to put in their own values to make it work. eg "\*\*inputdataset\*\*" would need to be exchanged for a dataset name.

</div>

# Set up the environment

<div class="alert alter-block alert-info">

R packages are a collection of functions developed by the open source community. They make it quicker to undertake analysis by providing additional functionality for common activities on top of base R's functionality.

There are thousands of packages available to choose from on [CRAN](https://cran.r-project.org/) - The Comprehensive R Achive Network).

Packages can be downloaded from CRAN using the command `install.packages("package-name").

After the package has been installed, it needs to be loaded to make it available for each program using `library(package-name)`.

</div>

The packages required for Part 1 are:

* tidyverse
* janitor
* sf

## Install and load required packages

Download these packages if they do not already exist locally and then load them.

The `require()` function will check if they are installed and load them. If not the following code will download them from CRAN and then load them.
```{r}

if (!require(tidyverse)){
  install.packages("tidyverse")
  library(tidyverse)
}

if (!require(janitor)){
  install.packages("janitor")
  library(janitor)
}

if (!require(sf)){
  install.packages("sf")
  library(sf)
}
```


<div class="alert alter-block alert-info">

The [tidyverse package](https://www.tidyverse.org/) is a collection of packages designed for data science.
The [janitor package](https://garthtarr.github.io/meatR/janitor.html) is a set of functions for examining and cleaning data
The [sf package](https://r-spatial.github.io/sf/) is designed to work with Simple Features which allow geographical and mapping data to be included in data frames.

</div>

## Set up the file path

<div class="alert alter-block alert-info">

All required input datasets are in a specific directory, called `data_lessons`.

It is necessary to find this folder and set up a variable that points directly to this directory.
All the input datasets can now be imported directly from this directory.

</div>

Assign the input_file_path to be the `data` directory
```{r}
input_file_path <- "/mypathtodata/data/"
```

# Read in CityFibre dataset

<div class="alert alter-block alert-info">

The initial dataset provided by CityFibre is called `cityfibre_scotland.csv`

</div>

Read the raw CityFibre data into a data frame using the read_csv() function from the `readr` tidyverse package. 
Give the raw data frame a descriptive name e.g. `cf_raw`
```{r}
cf_raw <- read_csv(file.path(input_file_path, "**myinputfile**.csv"))
```
## Tidy up column names

<div class="alert alter-block alert-info">

Use the `clean_names` function from the `janitor` package to tidy up the columns names so that it is easier to work with.
This will name the column names all lowercase and put an `_` between any multiple words

</div>

Apply the `clean_names()` function to the raw dataset.
Give the output a new name (eg `cf_input`)  as it has how been altered from the raw input file.
```{r}
**outputdataset** <- clean_names(**inputdataset**)
```

# Understanding the CityFibre dataset

## Visual inspection

The first thing to do when getting a dataset is to have a look at it.

<div class="alert alter-block alert-info">

The `head()` function shows the first few rows of a dataset.
The `print()` function can also be used.

</div>

Use `head()` to look at the first 10 rows of the dataset.
```{r}
head(**inputdataset**, **numberofrows**)
```
Experiment with using the `print()` function instead.
```{r}
print(**inputdataset**)
```
The difference is that the print function will show the whole dataset, whilst the head function just displays the requested number of rows.

## Size, shape and format

The next step is to understand the full size and shape of the dataset.

<div class="alert alter-block alert-info">

* The `str()` function shows the structure of the data frame.
* The `glipmse()` function which is a transposed version of the `print()` function can also be used.
* The `summary()` function is another method of displaying the contents of a data frame.

</div>

Use the `str()` function to display the dataset structure
```{r}
str(**inputdataset**)
```

Use the `glimpse()` function instead
```{r}
glimpse(**inputdataset**)
```
Use the `summary()` function instead
```{r}
summary(**inputdataset**)
```

<div class="alert alter-block alert-warning">

How many rows does the input CityFibre file have?

How many columns?

What information is represented in each column?

</div>

## Missing values

The next step is to identify whether any of the columns contain missing values.

<div class="alert alter-block alert-info">

There are two types of missing values in R:

* NA - Not Available. This is the more common method for representing missing data.
* NaN - Not a Number. This is returned for numerical values that cannot be calculated.

The generic function `is.na()` will return a boolean value for both type of missing.

Using sum(is.na()) will enable the number of `TRUE` responses to be summed.

</div>

Calculate number of missing values in each column of the `cf_input` dataset
```{r}
sum(is.na(cf_input$**var1**))
sum(is.na(cf_input$**var2**))
#Do this for all variables
```

<div class="alert alter-block alert-info">

Do any of the columns contain missing values?

</div>

## Unique values

The next step is to identify the number of unique values present in each column.

Use the `n_distinct()` function from the `dplyr` package to calculate the number of distinct values in each column of the `cf_input` dataset
```{r}
n_distinct(cf_input$**var1**)
n_distinct(cf_input$**var2**)
#Do this for all variables
```

<div class="alert alter-block alert-info">

How many different cities are represented in the data?

How many different nodes are there?

What does the difference between the number of postcodes and the size of the dataset imply?

</div>

## Column Relationships

<div class="alert alter-block alert-warning">

What are the distinct values for `city` and `city_code`? 

Is there a mapping or relationship between them?

</div>

Use the `tabyl()` function from the `janitor` package to compare the `city` and `city_code` values.
```{r}
tabyl(**inputdataset**, **var1**, **var2**)
```

<div class="alert alter-block alert-warning">

What does this say about the relationship between `city` and `city_code`?

Does the CityFibre dataset cover the whole of Scotland?

Is there a relationship between `city` and the `postcode` area?

</div>

<div class="alert alter-block alert-info">

Postcode area is the first one or two letters of the postcode string before the first numeric digit. More information about postcodes can be found [here](https://en.wikipedia.org/wiki/Postcodes_in_the_United_Kingdom).

This will need to be separated out from the postcode into an additional field.

A Regular Expression pattern can be used to extract the area code. The pattern used here starts at the beginning of the string, and matches any capital letter 0 or 1 times. A useful tool to test patterns can be found here: https://regexr.com/ 

`area_pattern <- "^[A-Z][A-Z]?"`

</div>

Use the `area_pattern` in a `mutate()` function to create a new `pc_area` field.
```{r}
area_pattern <- "^[A-Z][A-Z]?"

**inputdataset** <- mutate(**inputdataset**, pc_area = str_extract(postcode, area_pattern))
```

Use the `tabyl()` function as above to compare `pc_area` with `city_code`
```{r}
tabyl(**code to make tabyl work**)
```

<div class="alert alter-block alert-warning">

Does this highlight any issues?

What is going on with postcodes in Glasgow and Renfrewshire?

</div>

# Read in supporting datasets

<div class="alert alter-block alert-success">

There are a number of supporting datasets required for this analysis that can append additional information to the core CityFibre dataset.
Most of these datasets come with supporting information, so it is recommended to review the online information and data dictionaries alongside reading in the files.

These are:

1. The superfast broadband scheme voucher scheme
  + This provides postcode-level information about the voucher scheme available to help all properties to access superfast broadband
  
2. The Scottish Index of Multiple Deprivation 2020 indicators, indices and shapefiles
  + This is a tool for understanding relative deprivation of areas across Scotland
  + The file includes a relative level of deprivation for each datazone
  + The file includes the raw data indicators used to calculated the create the overall level
  + The file contains shapefiles the can be used to define the boundaries of the datazone on a map
  
3. Scottish postcode data, datazones and shapefiles
  + All postcodes in Scotland are split into two files: small user and large user. The large user postcodes cover single addresses receiving >1000 items per day. These are mainly business addresses. The small user postcodes cover on average 15 addresses and the shapefiles highlight the boundaries of these postcodes.
  + https://www.nrscotland.gov.uk/files//statistics/geography/2020-2/spd-datadictionary-2020-2.pdf
  + https://www.nrscotland.gov.uk/files/geography/Products/postcode-bkgrd-info.pdf
  + SIMD mapping between datazones and postcodes can also be extracted from these files
  + Datazones are areas larger than postcodes that are used to aggregate the census information to neighbourhood statistics. Each postcode can be mapped to its surrounding datazone.
  + https://www2.gov.scot/Topics/Statistics/sns/SNSRef/SNSPapDatZon

</div>

## Scottish Broadband Voucher Scheme

<div class="alert alter-block alert-info">

https://www.scotlandsuperfast.com/

https://www.scotlandsuperfast.com/how-can-i-get-it/voucher-scheme/

Scotland has a commitment to ensure every address in Scotland has access to a superfast broadband connection by the end of 2021. There is currently a voucher scheme in place for addresses where the current connection speed is less than 30Mbps. There are two types of voucher, £5000 for the main voucher scheme (MVS) and £400 for an interim scheme to help properties where the main rollout is in plan, but not until after 2021.

The `scheme_references.csv' has data provided by CityFibre at postcode level about the possible level of voucher available.

</div>

Read in the `scheme_references.csv` dataset and clean up names on input.
Call the dataset `sbvs_input`
```{r}
sbvs_input <- read_csv(file.path(input_file_path, "**inputfile**")) %>%
  clean_names()
```


## SIMD 2020

<div class="alert alter-block alert-info">

https://www.gov.scot/publications/scottish-index-multiple-deprivation-2020

The dataset has an identifier called `data_zone` which is larger than a postcode. More information on Data Zones can be found [here](https://www.isdscotland.org/Products-and-Services/GPD-Support/Geography/). Each Data Zone contains around 500-1000 people and are the smallest level for summarising the census information.

</div>

Before starting it is necessary to access the SIMD data which can be found here:
http://sedsh127.sedsh.gov.uk/Atom_data/ScotGov/ZippedShapefiles/SG_SIMD_2020.zip

Put this folder into the data folder.

Read in the  ".shp" dataset from the `SG_SIMD_2020` folder folder using the `st_read()` function from the `sf` package.
```{r}
simd_input <- st_read(file.path(input_file_path, "SG_SIMD_2020/SG_SIMD_2020.shp")) %>%
  clean_names()
```
The output is a special type of data frame with the boundary geometry data attached.

## Scottish postcode data

<div class="alert alter-block alert-info">

https://www.nrscotland.gov.uk/statistics-and-data/geography/our-products/scottish-postcode-directory/2020-2

Background information on postcodes can be found [here](https://www.nrscotland.gov.uk/files/geography/Products/postcode-bkgrd-info.pdf)
Postcodes are split into two types: small and large. 

Small user postcodes are based on one or more addresses. There are on average 15 delivery points in a single postcode with a boundary polygon around each postcode. The polygons cover the whole of Scotland's land surface.

Large user postcodes are allocated to single addresses that receive in excess of 1000 items of mail per day. There are no boundaries, but each large user is linked to its nearest small user postcode.

</div>

Before starting it is necessary to access the postcode information.
This can be the latest version if preferred.

Download the postcode unit boundaries, unzip and save the whole directory into the data directory.
https://www.nrscotland.gov.uk/files//statistics/geography/2020-2/PC_Cut_20_2.zip

Download the postcode indexes and save the small and large user files into the same directory
https://www.nrscotland.gov.uk/files//statistics/geography/2020-2/SPD_PostcodeIndex_Cut_20_2_CSV.zip

Using `read_csv()` read in the `SmallUser.csv` file, cleaning names on input. Call this `pc_small_input`
```{r}
**smallfile** <- **readfunction**(file.path(input_file_path, "PC_Cut_20_2/SmallUser.csv")) %>%
  clean_names()
```
Read in the `LargeUser.csv` file cleaning names on input. Call this `pc_large_input`.
```{r}
**largefile** <- **readfunction**(file.path(input_file_path, "PC_Cut_20_2/LargeUser.csv")) %>%
  clean_names()
```
All the supporting datasets have now being read in.

# Understanding supporting datasets
It is now necessary to understand the supporting datasets in more detail using a similar approach to the CityFibre dataset.

<div class="alert alter-block alert-info">

The steps required for each dataset are:

* Visual inspection
* Size, shape and format
* Missing values
* Unique values
* Relationships between columns

</div>

## Scottish Broadband Voucher Scheme

### Visual inspection
Use `head()` to have a quick look at the dataset to see what it contains.
```{r}
head(**sbvsfile**)
```
The file contains two columns. One with postcode and one containing the scheme reference.

### Size, shape and format
Use `glimpse()` and `summary()` to identify the number of rows, columns and the format of each
```{r}
glimpse(**sbvsfile**)
```
```{r}
summary(**sbvsfile**)
```

<div class="alert alter-block alert-warning">

How many rows are in the dataset?

How many columns?

What type of data is present?

</div>

### Missing values
Are there any missing values?
```{r}
sum(is.na(sbvs_input$**var1**))
sum(is.na(sbvs_input$**var2**))
```

<div class="alert alter-block alert-warning">

Are there any missing values present?

</div>

### Unique values
How many of the values are unique?
```{r}
n_distinct(sbvs_input$**var1**)
n_distinct(sbvs_input$**var2**)
```
<div class="alert alter-block alert-warning">

How many different values of postcode are present?

Do you expect this file to contain duplicates?

</div>

Use `tabyl()` to summarise the occurence of each different value of `scheme_references`
```{r}
#write a tabyl function here
```

<div class="alert alter-block alert-warning">

How many different values of `scheme_reference` are there?

What do they represent?

</div>

## SIMD 2020

### Visual inspection
Use `head()` to have a quick look at the dataset to see what it contains.
```{r}
#quick look at dataset
```
The SIMD file contains a lot of columns including some geometry information.

### Size, shape and format
Use `glimpse()` and `summary()` to identify the number of rows, columns and the format of each
```{r}
#glimpse and summary
```

<div class="alert alter-block alert-warning">

How many rows are in the dataset?

How many columns?

What type of data is present?

Are there any obvious data quality issues on visual inspection that will need to be addressed?

</div>

### Missing values
Look at the missing values for the key field which is `data_zone`.
```{r}
#use is.na() and sum() to look for missing values
```

<div class="alert alter-block alert-warning">

Are there any missing values for the `data_zone` variable?

</div>

### Unique values
Look at the unique values for the `data_zone` field.
```{r}
#use n_distinct() to calculate the unique values
```
<div class="alert alter-block alert-warning">

Are there duplicates in the data?

</div>

## Scottish postcode data

### Visual inspection
Use `head()` to have a quick look at both the small and large postcode files to see what they contain.
```{r}
#quick look at small and large user postcode files
```
Each file contains similar information, but there are differences between the two in terms of the columns and level of information.

### Size, shape and format
Use `glimpse()` and `summary()` to identify the number of rows, columns and the format of each postcode file.
```{r}
#small user glimpse and summary

#large user glimpse and summary
```

<div class="alert alter-block alert-warning">

How many rows are in each file?

How many columns?

Are the columns the same?

</div>

<div class="alert alter-block alert-info">

The small and large postcode files have different number of columns and therefore contain slightly different information.

It would be useful understand which columns occur in both datasets and which do not.

The `names()` function can be used to extract the column names into a separate vector for comparison.

Set operation functions `intersect()` and `setdiff()` can then be used to compare the two resulting vectors.

It is useful to know that `setdiff(A, B)` is not the same as `setdiff(B, A)`

</div>

Use the `names()` function to extract two vectors of column names for the two postcode files
```{r}
**vector1** <- names(**smallpostcodefile**)
**vector2** <- names(**largepostcodefile**)
```

Use the `intersect()` function to look at the columns in common between the two datasets
```{r}
intersect(**vector1**, **vector2**)
```

<div class="alert alter-block alert-warning">

How many columns are in common across both datasets?

Will all this information be needed for the analysis?

</div>

Use the `setdiff()` function twice to look at the columns that are not in common between the two vectors.
```{r}
setdiff(**vector1**, **vector2**)
setdiff(**vector2**, **vector1**)
```
<div class="alert alter-block alert-warning">

What information does the small user file contain that is not in the large user file?

What information does the large user file contain that is not in the small user file?

</div>

### Missing values

Look at missing values for both fields for the key field which is `postcode`
```{r}
# small user - sum the number of missing values of postcode
```

```{r}
# large user - sum the number of missing values of postcode
```
<div class="alert alter-block alert-warning">

Are there any missing values?

</div>

### Unique values
Look at distinct values of `postcode` for both files
```{r}
# small user - number of unique values
```

```{r}
# large user - number of unique values
```
<div class="alert alter-block alert-warning">

Are any of the postcodes duplicated?

Why might this be?

</div>
